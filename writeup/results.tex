\section{Results}
\label{sec:results}

%Performance Evaluation, Results, Discussion}

\textit{owners: Evan: graphs, All: interpretation (2.75 pages)}

\subsection{Scaling on Single Node}
  \label{sxn:results1}


   
  \vspace*{0.1in}

      In Table~\ref{tab:single_node}, we show the benefits of various
      optimizations described in
      Sec.~\ref{sxn:single_node_opt} on a single-node of our Spark system. 
      The test matrix $\mathcal{A}$ has {\it{m}} = 1.95M, {\it{n}} = 128K,
      {\it{s}} = 0.004, and {\it{nnz}} = 10$^9$. The rank parameter,
      {\it{k}} = 32. We started with a parallelized implementation,
      without any of the described optimizations, and measured the
      performance (in terms of time taken). We first implemented the
      multi-core synchronization scheme, wherein a single copy of the
      output matrix is maintained across all the threads (for the matrix multiplication).
      This resulted in a speedup of around 6.5X, primarily due to
      the reduction in the amount of data traffic between the
      last-level cache and main memory (there was around 19X measured reduction
      in traffic). We then implemented our cache blocking scheme,
      primarily targeted towards ensuring that the output of the
      matrix multiplication resides in the caches (since it is
      accessed and updated frequently). This led to a further 2.4X
     reduction in run-time, for an overall speedup of around 15.6X.

     Once the memory traffic was optimized for, we implemented our
     SIMD code, by vectorizing the element-row multiplication-add
     operations (described in detail in Sec.~\ref{sxn:single_node_opt}). 
     The resultant code sped up by a further 2.6X, for an overall
     speedup of 39.7X. Although the effective SIMD width
 ($\mathcal{S}$ = 4), there are overheads of address computation,
 stores, and not all computations were vectorized (QR code is still
 scalar).


 
  \begin{table}
  \begin{center}
  \begin{tabular}{ |c|c| } 
  \hline
  Single Node Optimization & Overall Speedup\\
  \hline
  Original Implementation & 1.0  \\
  Multi-Core Synchronization & 6.5 \\
  Cache Blocking & 15.6 \\
  SIMD & 39.7 \\
  \hline

  \end{tabular}
  \end{center}
  \caption{Single node optimizations to the CX C implementation and
  the subsequent speedup  each additional optimization provides.}
  \label{tab:single_node}
  \end{table}
 



  \subsection{Scaling across Multiple Nodes}
  \textcolor{red}{Mike R, Jatin: we need a narrative here}

  \subsubsection{CX Spark Phases}
  Our implementations of CX and PCA share the \textsc{RandomizedSVD} subroutine, which accounts for the bulk of the runtime and all of the distributed computations.
  The execution of \textsc{RandomizedSVD} proceeds in four distributed phases listed below, along with a small amount of additional local computation.
  \begin{enumerate}
      \item \textbf{Load Matrix Metadata}
         The dimensions of the matrix are read from the distributed filesystem to the driver.
      \item \textbf{Load Matrix}
         A distributed read is performed to load the matrix entries into an in-memory cached
         RDD containing one entry per row of the matrix.
      \item \textbf{Power Iterations}
         The \textsc{MultiplyGramian} loop on lines 2-5 of
         \textsc{RandomizedSVD} is run to compute an approximation $Q$
         of the dominant right singular subspace.
       \item \textbf{Finalization (Post-Processing)}
         Right multiplication by $Q$ on line 7 of \textsc{RandomizedSVD} to compute $C$.
  \end{enumerate}

  \subsubsection{Empirical Results}

    \begin{figure} [H]
    \begin{centering}
    \includegraphics[scale=0.4]{images/CX_Strong_Scaling_Rank_32_Partitions_default.pdf}
    \end{centering}
    \caption{ Strong scaling for the 4 phases of CX on an XC40 for 100GB dataset at rank 32 and default partitioning as concurrency is increased.} 
    \label{fig:xc40scaling}
    \end{figure} 

Figure~\ref{fig:xc40scaling} shows how the distributed Spark portion of our code scales as we add additional processors.  We considered 240, 480, and 960 cores.  An additional doubling (to 1920 cores) would be ineffective as there are only 1654 partitions, so many cores would remain unused.  In addition, with fewer partitions per core there are less opportunities for load balancing and speculative reexecution of slow tasks.

When we go from 240 to 480 cores, we achieve a speedup of approximately 1.6x from doubling the cores: 233 seconds versus 146 seconds.  However, as the number of partitions per core drops below two, and the amount of computation-per-core relative to communication overhead drops, the scaling slows down (as expected).  This results in a lower speedup of approximately 1.4x (146 seconds versus 102 seconds) when we again double the core count to 960.

  \subsection{Comparison across Multiple Platforms}
  \label{sect:h2h}
    
    \begin{figure} [H]
    \begin{centering}
    \includegraphics[scale=0.4]{images/CX_Size_Scaling_Rank_16_Partitions_default.pdf}
    \end{centering}
    \caption{ Run times for the various stages of computation for CX for two different dataset sizes for the three platforms using rank 16 and default partitioning for the given platform} 
    \label{fig:h2hrank16} 
    \end{figure}

    
  \input{h2hresults.tex}

  

  \subsection{Comparison of CX, PCA, RPCA quantitatively \textcolor{red}{Alex: please insert results here} }
    \begin{itemize}
      \item (for 100GB sized dataset on EC2)
      \item runtime vs. accuracy?
      \item show distinction b/w cx, pca
    \end{itemize}

  \subsection{Science plot for PCA, CX}
  Recall that CX decomposition samples actual columns from $A$ for low-rank approximation. In other words, it allows us to identify important columns based on which $A$ can be well reconstructed. Here, we examine the results of CX decomposition, i.e., columns sampled, on the MSI dataset.
 Rows and columns of our data matrix $A$ are corresponding to spacial pixels and $(\tau, m/z)$ value of ions, respectively. We apply CX decomposition on both $A$ and $A^T$ in order to identify important pixels and ions.
   
  In Figure~\ref{fig:cx_ions}, we present the distribution of the marginalized normalized ion leverage scores over $\tau$. That is, each score corresponds to an ion with $m/z$ value shown in the $x$-axis. Clearly, leverage scores of ions in three narrow regions have significantly larger magnitude than the rest. This indicates that these ions are more informative and should be kept as basis for reconstruction.  {\color{red} XXX: BEN, PLEASE COMMENT ON THE EXACT IONS FOUND. JEY MIGHT HAVE SENT YOU THE LIST M/Z VALUES. }
  
  On the other hand, we observed that the pixel leverage scores are fairly uniform (not shown here). This is not surprising since similar pixels tend to have similar and small individual scores. However, for each region that contains similar pixels, the total leverage score (sampling probability) will still be higher if such a region is more important in the reconstruction. Therefore, a random sampling is still able to capture the importance by sampling more pixels from the region as shown in Figure~\ref{fig:cx_spatial}.
  {\color{red} XXX: BEN, PLEASE COMMENT ON THESE REGIONS. }
    
    \begin{figure} [h!btp]
    \begin{centering}
    \includegraphics[width=\linewidth]{images/cx_ions.png}
    \end{centering}
    \caption{Normalized leverage scores (sampling probabilities) for $m/z$ marginalized over $\tau$.
      Three narrow regions of $m/z$ account for $59.3\%$ of the total probability mass.}
    \label{fig:cx_ions}
    \end{figure} 
    
    \begin{figure} [h!btp]
    \begin{centering}
    \includegraphics[width=\linewidth]{images/cx_spatial.png}
    \end{centering}
    \caption{Plot of 10000 points sampled by leverage score. Color and
      luminance of each point indicates density of points at that location as
      determined by a Gaussian kernel density estimate.}
    \label{fig:cx_spatial}
    \end{figure} 



  \subsection{Lessons Learned}
  \label{sect:lessons}
  
  \input{lessons.tex}

