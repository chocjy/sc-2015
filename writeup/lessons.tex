The difference in performance between the XC40 and the experimental Cray platform point to optimizations to Spark that could improve its performance on HPC-style architectures.  In particular:
\begin{itemize}
\item Spark is currently inefficient in cleaning up its local scratch space.  In particular, shuffle data is not immediately cleaned up after a shuffle completes.  This makes fault recovery more efficient, but results in higher storage requirements for scratch space.  If clean up was more efficient, it would be more feasible to fit all of the scratch data in a local RAM disk and not rely on Lustre at all.
\item Spark does not currently allow you to configure primary and backup scratch directories.  Instead you list all scratch directories in a single list, and it distributes data in a round round fashion between them as long as space is available.  You can bias it towards one storage device (e.g., RAM disk vs. Lustre) by listing multiple directories on the preferred device, but ideally we would like to use RAM disk exclusively unless and until it fills, and then switch to Lustre.
\item Spark does not allow you to specify that a scratch directory is globally accessible.  Thus non-chached data is stored to the remote Lustre directory by the sender, then later retrieved by the sender and sent to the receiver.  This wastes a step, since the receiver could easily fetch the data directly.
\item Alternatively, a push model of communication (as opposed to the current pull model) might be possible - however this would have implications for reliability and handling of very large data sets.\footnote{Storing the shuffle data to a large persistent block storage device, and only sending it as needed, allows Spark to send more data than could fit in a memory buffer on the remote side.}
\end{itemize}
