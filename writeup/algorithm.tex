\section{Low-rank matrix factorization}
\textit{Owners: Jiyan, Michael Mahoney (1 page)}

Low-rank matrix factorization is an important topic in linear algebra and numerical analysis.
It finds use in a variety of scientific fields, such as signal processing, pattern recognition, personalized recommendation.
More formally, we seek to find two or more smaller matrices such that their product is a well approximation to the original data matrix $A$.
That is,
\begin{equation}
 \label{eqn:apprx}
    \underset{m\times n}{A} \approx \underset{m\times k}{B} \times \underset{k\times n}{C}.
\end{equation}
In above, $B \times C$ is a rank-$k$ approximation to $A$.
Low-rank matrix factorization has the following advantages.
\begin{compactitem}
\item
It can be viewed as a dimension deduction technique.
In modern applications, datasets containing stupendously many rows or columns become more common, which makes it difficult for data visualization or applying classic algorithms. A low-rank approximate allows to use only a few features, e.g., $B$ is \eqref{eqn:apprx}, in the application.
\item
The results are more interpretable.
For example, in imaging analysis, the original images can be reconstructed using linear combination of basis images.
\item
It is useful in data compression.
Smaller matrices can be stored more efficiently.
\end{compactitem}

More formally, the quality of such a low-rank approximation is described as a data-dependent loss function $L$. A typical choice of $L$ is the squared loss. In this case, we seek to find
\begin{equation}
 \label{eqn:obj}
  \min_{\text{rank}(\tilde A) = k} \| A - \tilde A \|_F,
\end{equation}
where $\| \cdot \|_F$ is the Frobenius norm defined as $\|X\|_F = \sqrt{ \sum_{i=1}^m \sum_{j=1}^n X_{ij}^2 }$.
In the rest of this section, we outline two classes of matrix factorizations used in our application.

\subsection{SVD and PCA}
The singular value decomposition (SVD) is the factorization of matrix $A$ into the product of three matrices $U\Sigma V^T$ where $U$ and $V$ have orthonormal columns and $D$ is a diagonal matrix with positive real entries. The columns of $U$ and $V$ are called left and right singular vectors and the diagonal entries of $D$ are called singular values. For notation convenience, we assume the singular values are sorted such that $\sigma_1\geq \cdots \geq \sigma_r\geq 0$, and
this means that the columns of $U$ and $V$ are sorted by the order given by the singular values.  

%if optimality criterion to choose is the square loss ($\ell_2$ loss)
 
Importantly, for any target rank $k$, the solution to \eqref{eqn:obj} is given by the truncated SVD, i.e., $A_k = U_k \Sigma_k V_k^T$, where $U_k$ and $V_k$ contain the top $k$ singular vectors, i.e., the first $k$ columns of $U$ and $V$, respectively, and $\Sigma_k$ is a diagonal matrix containing the top-$k$ singular values.

Principal component analysis (PCA) and SVD are closely related.
PCA aims at converting the original features into a set of linearly uncorrelated variables called {\it principal components}.
To be more specific, the first principal component is defined as the direction along with the highest variance possible among the data points is attained, and each succeeding component in turn has the largest variance possible subject to the constraint that it is orthogonal to the preceding components.
Usually the number of principal components needed to preserve most of the information in $A$ is far less than the number of original features. Thus the goal of dimension reduction is achieved.

PCA is mathematically defined and can be found via SVD.
Assuming that matrix $A$ has been preprocessed, i.e., each column of the data matrix $A$ has been centered and has unit variance, the {\it loading} vectors are given by top-$k$ singular vectors $V_k$ which transform the original $n$ variables into $k$ new components. To be more specific, the top-$k$ principal components are given by 
\begin{equation}
  T_k = A V_k = \begin{pmatrix} \sigma_1 u_1 & \cdots & \sigma_n u_n \end{pmatrix}.
\end{equation}
Particularly, the $j$-th principal component of the $i$-th row of $A$ is given by 
\begin{equation}
  t_{ij} = a^i v_j = \sigma_j U_{ij}.
\end{equation}

As discussed above, SVD is fundamental in PCA. However, in general, it takes $\bigO(mnk)$ time to compute the truncated SVD, which becomes inapplicable when dealing with datasets of even moderately-large size, e.g., $m = 10^6$ and $n = 10^4$. Recently, Halko et al.~\cite{HMT11} propose to use randomized algorithms to construct a rank-$k$ approximation to $A$ which approximates $A$ nearly as well as $A_k$ does but need less flops and passes over the data matrix. This becomes extremely useful in many large-scale data analytics.


\subsection{CX}
As can be seen above, principal components are linear combinations of the original variables, which are less interpretable. A natural question arises: can we reconstruct the matrix using a few actual columns of $A$?

CX decomposition factorizes an $m \times n$ matrix $A$ into two matrices $C$ and $X$, where $C$ is an $m\times c$ matrix that consists of $c$ actual columns
of $A$, and $X$ is a $c \times n$ matrix such that $A\approx CX$.
%That is, linear combinations of the columns of $C$ can recover most of the ``information'' of the matrix $A$.
%A quantitative measurement of the closeness between $CX$ and $A$ is obtained by using the matrix Frobenius norm of the difference:
Using the same optimality criterion defined in~\eqref{eqn:obj}, we seek matrices $C$ and $X$ such that the residual error $\|A-CX\|_F$ is small.

Below, we outline the three basic steps. First, compute (either exactly or approximately) the {\it statistical leverage scores} of the columns of $A$;
and second, use those scores as a sampling distribution to select $c$ columns from $A$ and form $C$;
finally once the matrix $C$ is determined, the optimal matrix $X$ with rank-$k$ that minimizes $\|A-CX\|_F$ can be computed accordingly; see~\cite{DMM08} for detailed construction.

As we can see, central to CX decomposition is the underlying sampling distribution.
In the following, we give some intuition on our choice, i.e., leverage scores.

Given $A$ and a target rank parameter $k$, for $j=1,\ldots,n$, the $j$-th leverage score can be defined as
\begin{equation}
 \label{eqn:lev}
  \ell_j = \sum_{i=1}^k v_{ji}^2.
\end{equation}
These scores $\{\ell_j\}_{i=1}^{n}$ can be interpreted as how much ``leverage'' or ``influence'' the $j$-th column of $A$ exerts on the best rank-$k$ approximation to $A$. 
As mentioned above, $A_k = \sum_{i=1}^k \sigma_i u_i v_i^T$ gives the best rank-$k$ approximation to $A$.
In fact, $A_k$ can be viewed as the projection of $A$ onto the top-$k$ \emph{left} singular space spanned by the columns of $\begin{pmatrix} u_1 & \cdots & u_k \end{pmatrix}$.
Since multiplying each column by the corresponding singular value does not alter the subspace, we can view
$\begin{pmatrix} \sigma_1 u_1 & \cdots & \sigma_k u_k \end{pmatrix}$ as a basis for this space.  
Especially, if $A$ has been preprocessed, these vectors are its top-$k$ principal components.
Then, for each column of $A$, we have that
  $$  a_j = \sum_{i=1}^{\text{rank}(A)} (\sigma_i u_i) v_{ji} \approx \sum_{i=1}^k (\sigma_i u_i) v_{ji}.  $$
That is, the $j$-th column of $A$ can be expressed as a linear combination of the basis of the top-$k$ left singular space with $v_{ji}$ as the coefficients.
%On the other hand, the scores $\{\ell_j\}_{j=1}^{n}$ equal the diagonal elements of the projection matrix onto the top-$k$ \emph{right} singular subspace spanned by $\begin{pmatrix} v_1 & \cdots & v_k \end{pmatrix}$.
%, and thus these statistical leverage scores are a generalization of the diagonal elements of the ``hat matrix'' in regression diagnostics~\cite{MD09}.
For $j=1,\ldots,n$, if we define the {\it normalized leverage scores} as
\begin{equation}
\label{eqn:nlev}
  p_j = \frac{\ell_j}{\sum_{i=1}^n \ell_i},
\end{equation}      
where $\ell_i$ is defined in~\eqref{eqn:lev} and choose columns from $A$ according to those normalized leverage scores, then one should expect
the selected columns are able to reconstruct the matrix $A$ nearly as well as $A_k$ does.

As computing the exact truncated SVD in prohibitive data with massive size, 
in this work, we compute the {\it approximate} leverage scores, which are leverage scores of a matrix $\tilde A_k$ which is close to $A_k$. As mentioned above, such an approximation $\tilde A_k$ can be constructed using randomized SVD algorithm~\cite{HMT11} which runs in significantly less time and needs less number of passes over the data matrix. This idea is originally proposed by Drineas et al.~\cite{DMMW12_JMLR}. 

%These algorithms compute high-quality approximations to the normalized leverage scores of the input matrix, and the running time of these algorithms depends on the time to apply a random projection to the input matrix, which is much faster than computing the full (or even a truncated) SVD.

 \begin{algorithm}[tb]
 \caption{CX Decomposition}
  \label{alg:cx}
  \begin{algorithmic}[1]
    \Require $A \in \reals^{m\times n}$, rank parameter $k \leq \textrm{rank}(A)$, number of power iterations $q$.

    \Ensure $C$.
    
    %\Function{ColSelect}{$A,k,r$}

    \State Compute an approximation of the top-$k$ right singular vectors of $A$ denoted by $\tilde V_k$, using randomized SVD described in~\cite{HMT11} with $q$ power iterations.
    
    \State Let $\ell_i = \| \tilde V_k^i\|_2$, where $V_k^i$ is the $i$-th row of $V_k$, for $i = 1, \ldots, n$. 
    
    \State Define $p_i = \ell_i / \sum_{j=1}^d \ell_j$ for $i=1,\ldots,n$.
    
    \State Sample $c$ columns based on $\{p_i\}_{i=1}^n$.

    %\State \Return The indices with the top $r$ $\ell_i$s.
    
    %\EndFunction

    \end{algorithmic}
\end{algorithm}



