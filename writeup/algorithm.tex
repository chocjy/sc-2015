\section{Methods: Low-rank matrix factorization}
\label{sxn:low-rank-methods}
\textit{Owners: Jiyan, Michael Mahoney (1 page)}

Low-rank matrix factorization is an important topic in linear algebra and numerical analysis.
It finds use in a variety of scientific fields, such as signal processing, pattern recognition, personalized recommendation.
More formally, we seek to find two or more smaller matrices such that their product is a well approximation to the original data matrix $A$.
That is,
\begin{equation}
 \label{eqn:apprx}
    \underset{m\times n}{A} \approx \underset{m\times k}{B} \times \underset{k\times n}{C}.
\end{equation}
In above, $B \times C$ is a rank-$k$ approximation to $A$.
Low-rank matrix factorization has the following advantages.
\begin{compactitem}
\item
It can be viewed as a dimension deduction technique.
In modern applications, datasets containing stupendously many rows or columns become more common, which makes it difficult for data visualization or applying classic algorithms. A low-rank approximate allows to use only a few features, e.g., $B$ is \eqref{eqn:apprx}, in the application.
\item
The results are more interpretable.
For example, in imaging analysis, the original images can be reconstructed using linear combination of basis images.
\item
It is useful in data compression.
Smaller matrices can be stored more efficiently.
\end{compactitem}

More formally, the quality of such a low-rank approximation is described as a data-dependent loss function $L$. A typical choice of $L$ is the squared loss. In this case, we seek to find
\begin{equation}
 \label{eqn:obj}
  \min_{\text{rank}(\tilde A) = k} \| A - \tilde A \|_F,
\end{equation}
where $\| \cdot \|_F$ is the Frobenius norm defined as $\|X\|_F = \sqrt{ \sum_{i=1}^m \sum_{j=1}^n X_{ij}^2 }$.

Depending on the application, various low-rank factorization techniques have been exploited by practitioners. Popular choices include principal component analysis~\cite{pcaBook}, rank-revealing QR factorization~\cite{GE96}, nonnegative matrix factorization~\cite{NMFalg}, CUR/CX decomposition~\cite{CUR_PNAS}.
In this work, we consider using PCA and CX decomposition for our scalable and interpretable data analysis, which are briefly outlined in the rest of this section. Throughout the rest of the section, for an arbitrary matrix $A$, denote by $\a_i$ its $i$-th column, $\a^j$ its $j$-th row and $\a_{ij}$ its $(i,j)$-th element. Hereby, we assume the data matrix $A$ has size $m$ by $n$ and rank $r$.

\subsection{SVD and PCA}
The singular value decomposition (SVD) is the factorization of matrix $A \in \reals^{m\times n}$ into the product of three matrices $U\Sigma V^T$ where $U \in \reals^{m\times r}$ and $V\in \reals^{r\times n}$ have orthonormal columns and $D\in \reals^{r\times r}$ is a diagonal matrix with positive real entries. The columns of $U$ and $V$ are called left and right singular vectors and the diagonal entries of $D$ are called singular values. For notation convenience, we assume the singular values are sorted such that $\sigma_1\geq \cdots \geq \sigma_r\geq 0$, and
this means that the columns of $U$ and $V$ are sorted by the order given by the singular values.  

%if optimality criterion to choose is the square loss ($\ell_2$ loss)
 
Importantly, for any target rank $k\leq r$, the solution to \eqref{eqn:obj} is given by the truncated SVD, i.e., $A_k = U_k \Sigma_k V_k^T$, where $U_k \in \reals^{m\times k}$ and $V_k\in \reals^{n\times k}$ contain the top $k$ singular vectors, i.e., the first $k$ columns of $U$ and $V$, respectively, and $\Sigma_k\in \reals^{k\times k}$ is a diagonal matrix containing the top-$k$ singular values.

Principal component analysis (PCA) and SVD are closely related.
PCA aims at converting the original features into a set of linearly uncorrelated variables called {\it principal components}.
To be more specific, the first principal component is defined as the direction along with the highest variance possible among the data points is attained, and each succeeding component in turn has the largest variance possible subject to the constraint that it is orthogonal to the preceding components.
Usually the number of principal components needed to preserve most of the information in $A$ is far less than the number of original features. Thus the goal of dimension reduction is achieved.

PCA is mathematically defined and can be found via SVD.
Assuming that matrix $A$ has been preprocessed, i.e., each column of the data matrix $A$ has been centered and has unit variance, the {\it loading} vectors are given by top-$k$ singular vectors $V_k$ which transform the original $n$ variables into $k$ new components. To be more specific, the top-$k$ principal components are given by 
\begin{equation}
  T_k = A V_k = \begin{pmatrix} \sigma_1 \u_1 & \cdots & \sigma_n \u_n \end{pmatrix}.
\end{equation}
Particularly, the $j$-th principal component of the $i$-th row of $A$ is given by 
\begin{equation}
  t_{ij} = \a^i \v_j = \sigma_j \u_{ij}.
\end{equation}

\subsection{CX}
As can be seen above, principal components are linear combinations of the original variables, which are less interpretable. A natural question arises: can we reconstruct the matrix using a few actual columns of $A$?

CX decomposition factorizes an $m \times n$ matrix $A$ into two matrices $C$ and $X$, where $C$ is an $m\times c$ matrix that consists of $c$ actual columns
of $A$, and $X$ is a $c \times n$ matrix such that $A\approx CX$.
%That is, linear combinations of the columns of $C$ can recover most of the ``information'' of the matrix $A$.
%A quantitative measurement of the closeness between $CX$ and $A$ is obtained by using the matrix Frobenius norm of the difference:
Using the same optimality criterion defined in~\eqref{eqn:obj}, we seek matrices $C$ and $X$ such that the residual error $\|A-CX\|_F$ is small.

Below, we outline the three basic steps. First, compute (either exactly or approximately) the {\it statistical leverage scores} of the columns of $A$;
and second, use those scores as a sampling distribution to select $c$ columns from $A$ and form $C$;
finally once the matrix $C$ is determined, the optimal matrix $X$ with rank-$k$ that minimizes $\|A-CX\|_F$ can be computed accordingly; see~\cite{DMM08} for detailed construction.

As we can see, central to CX decomposition is the underlying sampling distribution.
In the following, we give some intuition on our choice, i.e., leverage scores.

Let $A=U\Sigma V^T$ be the SVD of $A$.
Given a target rank parameter $k$, for $j=1,\ldots,n$, the $j$-th leverage score can be defined as
\begin{equation}
 \label{eqn:lev}
  \ell_j = \sum_{i=1}^k \v_{ji}^2.
\end{equation}
These scores $\{\ell_j\}_{i=1}^{n}$ can be interpreted as how much ``leverage'' or ``influence'' the $j$-th column of $A$ exerts on the best rank-$k$ approximation to $A$. 
As mentioned above, $A_k = \sum_{i=1}^k \sigma_i u_i v_i^T$ gives the best rank-$k$ approximation to $A$.
In fact, $A_k$ can be viewed as the projection of $A$ onto the top-$k$ \emph{left} singular space spanned by the columns of $\begin{pmatrix} u_1 & \cdots & u_k \end{pmatrix}$.
Since multiplying each column by the corresponding singular value does not alter the subspace, we can view
$\begin{pmatrix} \sigma_1 u_1 & \cdots & \sigma_k u_k \end{pmatrix}$ as a basis for this space.  
Especially, if $A$ has been preprocessed, these vectors are its top-$k$ principal components.
Then, for each column of $A$, we have that
  $$  a_j = \sum_{i=1}^{r} (\sigma_i u_i) v_{ji} \approx \sum_{i=1}^k (\sigma_i u_i) v_{ji}.  $$
That is, the $j$-th column of $A$ can be expressed as a linear combination of the basis of the top-$k$ left singular space with $v_{ji}$ as the coefficients.
%On the other hand, the scores $\{\ell_j\}_{j=1}^{n}$ equal the diagonal elements of the projection matrix onto the top-$k$ \emph{right} singular subspace spanned by $\begin{pmatrix} v_1 & \cdots & v_k \end{pmatrix}$.
%, and thus these statistical leverage scores are a generalization of the diagonal elements of the ``hat matrix'' in regression diagnostics~\cite{MD09}.
For $j=1,\ldots,n$, if we define the {\it normalized leverage scores} as
\begin{equation}
\label{eqn:nlev}
  p_j = \frac{\ell_j}{\sum_{i=1}^n \ell_i},
\end{equation}      
where $\ell_i$ is defined in~\eqref{eqn:lev} and choose columns from $A$ according to those normalized leverage scores, then one should expect
the selected columns are able to reconstruct the matrix $A$ nearly as well as $A_k$ does.

To compute the leverage scores based on \eqref{eqn:lev}, one needs to compute the top $k$ right-singular vectors $V_k$. As pointed out above, this is prohibitive data with massive size.
However, like in PCA, one can use randomized SVD as a remedy.
That is, in this work, we compute the {\it approximate} leverage scores, which are computed based on the randomized SVD algorithm~\cite{HMT09_SIREV}.
This approach requires less flops and less number of passes over the data matrix to obtain $\{\ell_i\}_{i=1}^n$, and it is originally proposed by Drineas et al.~\cite{DMMW12_JMLR}.
We summarize the major steps of CX decomposition using approximate leverage scores in Algorithm~\ref{alg:cx}.

%Finally, we want to point out that,
%although delivering different low-rank factorizations, both PAC and CX suffer from the fact that they need to truncated SVD.
%To make the algorithms practical on large-scale dataset, one can alleviate the demanding complexity by using randomized SVD. 

%These algorithms compute high-quality approximations to the normalized leverage scores of the input matrix, and the running time of these algorithms depends on the time to apply a random projection to the input matrix, which is much faster than computing the full (or even a truncated) SVD.

 \begin{algorithm}[tb]
 \caption{CX Decomposition}
  \label{alg:cx}
  \begin{algorithmic}[1]
    \Require $A \in \reals^{m\times n}$, rank parameter $k \leq \textrm{rank}(A)$, number of power iterations $q$.

    \Ensure $C$.
    
    %\Function{ColSelect}{$A,k,r$}

    \State Compute an approximation of the top-$k$ right singular vectors of $A$ denoted by $\tilde V_k$, using \textsc{RandomizedSVD} with $q$ power iterations.
    
    \State Let $\ell_i = \sum_{j=1}^k \tilde \v_{ij}^2$, where $\tilde \v_{ij}^2$ is the $(i,j)$-th element of $\tilde V_k$, for $i = 1, \ldots, n$. 
    
    \State Define $p_i = \ell_i / \sum_{j=1}^d \ell_{ij}$ for $i=1,\ldots,n$.
    
    \State Sample $c$ columns based on $\{p_i\}_{i=1}^n$.

    %\State \Return The indices with the top $r$ $\ell_i$s.
    
    %\EndFunction

    \end{algorithmic}
\end{algorithm}

\subsection{Randomized SVD}
As discussed above, SVD is fundamental to both PCA and CX. However, in general, to compute the truncated SVD with rank $k$, the required complexity is $\bigO(mnk)$ and it needs $\bigO(k)$ passes over the dataset, which becomes prohibitively expensive when dealing with datasets of even moderately-large size, e.g., $m = 10^6$, $n = 10^4$ and $k = 20$. Recently, Halko et al.~\cite{HMT09_SIREV} propose to use randomized algorithms to construct a rank-$k$ approximation to $A$ which approximates $A$ nearly as well as $A_k$ does.
The basis of the approximation is computed based on a smaller matrix after applying a random projection on $A$ that with high probability the range space of $A_k$ is preserved. We refer the readers to \cite{HMT09_SIREV,Mah-mat-rev_BOOK} for more details.
Importantly, it runs in $\bigO(mn \log k)$ time and needs only a constant number of passes over the data matrix. These properties becomes extremely desirable in many large-scale data analytics. We call this approach as {\it randomized SVD} and its major steps are summarized in Algorithm~\ref{alg:rsvd}.
 As can be seen, the cost is dominated by the matrix-matrix multiplication appeared in step 3\&6 in Algorithm~\ref{alg:rsvd}, which involve making pass over the entire data matrix.
However, they can be parallelized and hence randomized SVD is well amenable to distributed computing.~\footnote{It is worth mentioning that, step 3 in Algorithm~\ref{alg:rsvd} involves with dense matrix multiplication which incurs more flops but is more suitable for distributed environments whereas in \cite{HMT09_SIREV} they propose to use an FFT-based transform in order to get a faster asymptotic rate.}


\begin{algorithm}[tb]
 \caption{{\sc RandomizedSVD} Algorithm}
  \label{alg:rsvd}
  \begin{algorithmic}[1]
    \Require $A \in \reals^{m\times n}$, \
      number of power iterations $q \ge 1$, \
      and rank $r > 0$, slack $\ell \ge 0$ such that $k=r+\ell \leq \operatorname{rank}(A)$.

    % TODO: domain of each result
    \Ensure $U \Sigma V^T \approx \Call{ThinSVD}{A, r}$.

    \State Initialize $B \in \reals^{n\times k}$ by sampling $B_{ij} \sim \mathcal{N}(0, 1)$.

    \For{$q$ times}
        \State $B \gets \Call{MultiplyGramian}{A, B}$
        \State $(B, \_) \gets \Call{ThinQR}{B}$
    \EndFor

    \State Let $Q$ be the first $r$ columns of $B$.

    \State Let $C = \Call{Multiply}{A, Q}$.

    \State Compute $(U, \Sigma, \tilde V^T) = \Call{ThinSVD}{C}$.

    \State Let $V = Q \tilde V$.
    % alternative that doesn't require knowing the distributivity of transpose but looks silly:
    %\State Let $V^T = \tilde V^T Q^T$.

    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[tb]
  \caption{{\sc MultiplyGramian} Algorithm}
  \label{alg:gram}
  \begin{algorithmic}[1]
    \Require $A \in \reals^{m\times n}$, $B \in \reals^{n\times k}$.
    \Ensure $X = A^T A B$.
    \State Initialize $X = 0$.
    \For{each row $a$ in $A$}
        \State $X \gets X + a a^T B$.
    \EndFor
  \end{algorithmic}
\end{algorithm}


